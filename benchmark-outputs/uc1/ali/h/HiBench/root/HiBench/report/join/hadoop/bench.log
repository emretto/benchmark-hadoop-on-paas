SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/apps/ecm/service/hadoop/2.8.5-1.6.1/package/hadoop-2.8.5-1.6.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/root/HiBench/hadoopbench/sql/target/apache-hive-0.14.0-bin/lib/hive-jdbc-0.14.0-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
20/12/21 15:55:50 WARN conf.HiveConf: HiveConf of name hive.stats.column.autogather does not exist
20/12/21 15:55:50 WARN conf.HiveConf: HiveConf of name hive.mapjoin.hybridgrace.hashtable does not exist
20/12/21 15:55:50 WARN conf.HiveConf: HiveConf of name hive.blobstore.use.blobstore.as.scratchdir does not exist
20/12/21 15:55:50 WARN conf.HiveConf: HiveConf of name hive.merge.sparkfiles does not exist
20/12/21 15:55:50 WARN conf.HiveConf: HiveConf of name hive.blobstore.optimizations.enabled does not exist
20/12/21 15:55:50 WARN conf.HiveConf: HiveConf of name hive.optimize.dynamic.partition.hashjoin does not exist
20/12/21 15:55:50 WARN conf.HiveConf: HiveConf of name hive.blobstore.supported.schemes does not exist

Logging initialized using configuration in jar:file:/root/HiBench/hadoopbench/sql/target/apache-hive-0.14.0-bin/lib/hive-common-0.14.0.jar!/hive-log4j.properties
OK
Time taken: 0.592 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.196 seconds
OK
Time taken: 2.291 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.087 seconds
Query ID = root_20201221155555_7db39b2a-be97-4de1-83f2-6d733dbfa758
Total jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Defaulting to jobconf value of: 12
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1608498255007_0070, Tracking URL = http://emr-header-1.cluster-53391:20888/proxy/application_1608498255007_0070/
Kill Command = /usr/lib/hadoop-current/bin/hadoop job  -kill job_1608498255007_0070
Hadoop job information for Stage-1: number of mappers: 36; number of reducers: 12
2020-12-21 15:56:02,211 Stage-1 map = 0%,  reduce = 0%
2020-12-21 15:56:27,164 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 39.91 sec
2020-12-21 15:56:29,242 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 101.78 sec
2020-12-21 15:56:30,275 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 125.66 sec
2020-12-21 15:56:33,381 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 158.06 sec
2020-12-21 15:56:34,412 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 166.85 sec
2020-12-21 15:56:35,448 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 179.73 sec
2020-12-21 15:56:36,480 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 183.25 sec
2020-12-21 15:56:39,582 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 203.93 sec
2020-12-21 15:56:40,612 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 209.37 sec
2020-12-21 15:56:41,653 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 218.81 sec
2020-12-21 15:56:44,770 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 222.69 sec
2020-12-21 15:56:45,807 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 244.43 sec
2020-12-21 15:56:47,883 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 260.56 sec
2020-12-21 15:56:49,960 Stage-1 map = 44%,  reduce = 0%, Cumulative CPU 273.27 sec
2020-12-21 15:56:51,000 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 276.54 sec
2020-12-21 15:56:53,064 Stage-1 map = 46%,  reduce = 0%, Cumulative CPU 281.44 sec
2020-12-21 15:56:54,093 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 290.92 sec
2020-12-21 15:56:56,151 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 308.04 sec
2020-12-21 15:56:57,183 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 335.01 sec
2020-12-21 15:56:58,222 Stage-1 map = 66%,  reduce = 0%, Cumulative CPU 349.73 sec
2020-12-21 15:56:59,252 Stage-1 map = 73%,  reduce = 0%, Cumulative CPU 367.6 sec
2020-12-21 15:57:00,279 Stage-1 map = 76%,  reduce = 0%, Cumulative CPU 375.25 sec
2020-12-21 15:57:01,316 Stage-1 map = 80%,  reduce = 0%, Cumulative CPU 384.69 sec
2020-12-21 15:57:02,345 Stage-1 map = 81%,  reduce = 0%, Cumulative CPU 386.03 sec
2020-12-21 15:57:03,385 Stage-1 map = 83%,  reduce = 0%, Cumulative CPU 393.27 sec
2020-12-21 15:57:09,590 Stage-1 map = 86%,  reduce = 0%, Cumulative CPU 400.21 sec
2020-12-21 15:57:10,622 Stage-1 map = 89%,  reduce = 5%, Cumulative CPU 407.23 sec
2020-12-21 15:57:17,819 Stage-1 map = 100%,  reduce = 7%, Cumulative CPU 436.75 sec
2020-12-21 15:57:18,848 Stage-1 map = 100%,  reduce = 13%, Cumulative CPU 437.93 sec
2020-12-21 15:57:19,883 Stage-1 map = 100%,  reduce = 19%, Cumulative CPU 441.29 sec
2020-12-21 15:57:21,941 Stage-1 map = 100%,  reduce = 32%, Cumulative CPU 453.26 sec
2020-12-21 15:57:22,968 Stage-1 map = 100%,  reduce = 58%, Cumulative CPU 475.85 sec
2020-12-21 15:57:26,057 Stage-1 map = 100%,  reduce = 75%, Cumulative CPU 486.26 sec
2020-12-21 15:57:28,113 Stage-1 map = 100%,  reduce = 92%, Cumulative CPU 495.57 sec
2020-12-21 15:57:29,138 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 500.2 sec
MapReduce Total cumulative CPU time: 8 minutes 20 seconds 200 msec
Ended Job = job_1608498255007_0070
Launching Job 2 out of 3
Number of reduce tasks not specified. Defaulting to jobconf value of: 12
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1608498255007_0071, Tracking URL = http://emr-header-1.cluster-53391:20888/proxy/application_1608498255007_0071/
Kill Command = /usr/lib/hadoop-current/bin/hadoop job  -kill job_1608498255007_0071
Hadoop job information for Stage-2: number of mappers: 12; number of reducers: 12
2020-12-21 15:57:37,039 Stage-2 map = 0%,  reduce = 0%
2020-12-21 15:57:51,604 Stage-2 map = 17%,  reduce = 0%, Cumulative CPU 7.6 sec
2020-12-21 15:57:52,641 Stage-2 map = 33%,  reduce = 0%, Cumulative CPU 15.11 sec
2020-12-21 15:57:53,672 Stage-2 map = 42%,  reduce = 0%, Cumulative CPU 19.17 sec
2020-12-21 15:57:57,812 Stage-2 map = 58%,  reduce = 0%, Cumulative CPU 25.94 sec
2020-12-21 15:57:58,848 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 43.45 sec
2020-12-21 15:58:01,944 Stage-2 map = 100%,  reduce = 17%, Cumulative CPU 49.27 sec
2020-12-21 15:58:02,981 Stage-2 map = 100%,  reduce = 42%, Cumulative CPU 58.7 sec
2020-12-21 15:58:04,025 Stage-2 map = 100%,  reduce = 50%, Cumulative CPU 61.7 sec
2020-12-21 15:58:08,179 Stage-2 map = 100%,  reduce = 75%, Cumulative CPU 70.89 sec
2020-12-21 15:58:09,206 Stage-2 map = 100%,  reduce = 92%, Cumulative CPU 76.73 sec
2020-12-21 15:58:10,238 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 79.68 sec
MapReduce Total cumulative CPU time: 1 minutes 19 seconds 680 msec
Ended Job = job_1608498255007_0071
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1608498255007_0072, Tracking URL = http://emr-header-1.cluster-53391:20888/proxy/application_1608498255007_0072/
Kill Command = /usr/lib/hadoop-current/bin/hadoop job  -kill job_1608498255007_0072
Hadoop job information for Stage-3: number of mappers: 12; number of reducers: 1
2020-12-21 15:58:17,886 Stage-3 map = 0%,  reduce = 0%
2020-12-21 15:58:33,325 Stage-3 map = 42%,  reduce = 0%, Cumulative CPU 17.51 sec
2020-12-21 15:58:37,434 Stage-3 map = 50%,  reduce = 0%, Cumulative CPU 20.42 sec
2020-12-21 15:58:38,461 Stage-3 map = 75%,  reduce = 0%, Cumulative CPU 29.39 sec
2020-12-21 15:58:39,487 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 38.6 sec
2020-12-21 15:58:42,580 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 43.75 sec
MapReduce Total cumulative CPU time: 43 seconds 750 msec
Ended Job = job_1608498255007_0072
Loading data to table default.rankings_uservisits_join
MapReduce Jobs Launched:
Stage-Stage-1: Map: 36  Reduce: 12   Cumulative CPU: 500.2 sec   HDFS Read: 1919313244 HDFS Write: 12714655 SUCCESS
Stage-Stage-2: Map: 12  Reduce: 12   Cumulative CPU: 79.68 sec   HDFS Read: 12719683 HDFS Write: 11282969 SUCCESS
Stage-Stage-3: Map: 12  Reduce: 1   Cumulative CPU: 43.75 sec   HDFS Read: 11287997 HDFS Write: 10009388 SUCCESS
Total MapReduce CPU Time Spent: 10 minutes 23 seconds 630 msec
OK
Time taken: 168.939 seconds
